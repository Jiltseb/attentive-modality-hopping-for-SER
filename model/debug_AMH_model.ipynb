{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data\n",
    "- get_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from process_data_AMH import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data : train_audio_mfcc.npy train_audio_seqN.npy train_audio_prosody.npytrain_nlp_trans.npy  train_video.npy train_video_seqN.npy train_label.npy\n",
      "[completed] load data\n",
      "load data : dev_audio_mfcc.npy dev_audio_seqN.npy dev_audio_prosody.npydev_nlp_trans.npy  dev_video.npy dev_video_seqN.npy dev_label.npy\n",
      "[completed] load data\n",
      "load data : test_audio_mfcc.npy test_audio_seqN.npy test_audio_prosody.npytest_nlp_trans.npy  test_video.npy test_video_seqN.npy test_label.npy\n",
      "[completed] load data\n"
     ]
    }
   ],
   "source": [
    "batch_gen = ProcessDataAMH(data_path='../data/target_seven_120/fold01/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6058, 7)\n",
      "(404, 7)\n",
      "(1025, 7)\n"
     ]
    }
   ],
   "source": [
    "print (np.shape( batch_gen.train_set ))\n",
    "print (np.shape( batch_gen.dev_set ))\n",
    "print (np.shape( batch_gen.test_set ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 120)\n",
      "()\n",
      "(35,)\n",
      "(128,)\n",
      "(32, 2048)\n",
      "()\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "for i in range(7):\n",
    "    print (np.shape(batch_gen.train_set[0][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AMH model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data : train_audio_mfcc.npy train_audio_seqN.npy train_audio_prosody.npytrain_nlp_trans.npy  train_video.npy train_video_seqN.npy train_label.npy\n",
      "[completed] load data\n",
      "load data : dev_audio_mfcc.npy dev_audio_seqN.npy dev_audio_prosody.npydev_nlp_trans.npy  dev_video.npy dev_video_seqN.npy dev_label.npy\n",
      "[completed] load data\n",
      "load data : test_audio_mfcc.npy test_audio_seqN.npy test_audio_prosody.npytest_nlp_trans.npy  test_video.npy test_video_seqN.npy test_label.npy\n",
      "[completed] load data\n"
     ]
    }
   ],
   "source": [
    "from model_AMH import *\n",
    "from process_data_AMH import *\n",
    "\n",
    "from train_AMH import *\n",
    "batch_gen = ProcessDataAMH(data_path='../data/target_seven_120/fold01/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1204 22:12:00.219098 139946112415488 deprecation.py:323] From /media/disk1/dato_space/attentive-modality-hopping-for-SER/model/layers.py:13: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W1204 22:12:00.223741 139946112415488 deprecation.py:323] From /media/disk1/dato_space/attentive-modality-hopping-for-SER/model/layers.py:46: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "W1204 22:12:00.226301 139946112415488 deprecation.py:323] From /media/disk1/dato_space/attentive-modality-hopping-for-SER/model/layers.py:92: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[object created]  ModelAMH\n",
      "[launch-multi] create audio/text/video model\n",
      "[object created]  ModelAudio\n",
      "[launch-audio] placeholders\n",
      "[launch-audio] create gru cell - bidirectional: 0\n",
      "[INFO] IS_AUDIO_RESIDUAL:  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1204 22:12:00.629476 139946112415488 deprecation.py:506] From /home/dato/anaconda2/envs/tf114_p36/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:564: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1204 22:12:00.644845 139946112415488 deprecation.py:506] From /home/dato/anaconda2/envs/tf114_p36/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:574: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1204 22:12:00.909729 139946112415488 deprecation.py:323] From /home/dato/anaconda2/envs/tf114_p36/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[object created]  ModelText\n",
      "[launch-text] placeholders\n",
      "[launch-text] create embedding\n",
      "[launch-text] use pre-trained embedding\n",
      "[launch-text] create gru cell - bidirectional: 0\n",
      "[object created]  ModelVideo\n",
      "[launch-video] placeholders\n",
      "[launch-video] create gru cell - bidirectional: 0\n",
      "[launch-multi] placeholders\n",
      "all the three modalities are used\n",
      "[launch-multi-attn] create an attention layer: A(1)+T(2) --> V(3)\n",
      "[launch-multi-attn] create an attention layer: T(2)+V2(3) --> A(1)\n",
      "[launch-multi-attn] create an attention layer: A2(1)+V2(3) --> T(2)\n",
      "[launch-multi-attn] create an attention layer: A(1)+T(2) --> V(3)\n",
      "[launch-multi-attn] create an attention layer: T(2)+V2(3) --> A(1)\n",
      "[launch-multi-attn] create an attention layer: A2(1)+V2(3) --> T(2)\n",
      "[launch-multi-attn] create an attention layer: A(1)+T(2) --> V(3)\n",
      "[launch-multi] create output projection layer\n",
      "[launch-multi] create optimizer\n",
      "[launch-multi] create summary\n"
     ]
    }
   ],
   "source": [
    "model_multi = ModelAMH(\n",
    "                    batch_size = 128,\n",
    "                    lr = 0.001,\n",
    "                    type_modality = 0,\n",
    "                    hop=7,\n",
    "                    encoder_size_audio = 750,      # Audio\n",
    "                    num_layer_audio = 1,\n",
    "                    hidden_dim_audio = 200,                \n",
    "                    dr_audio = 1.0,\n",
    "                    bi_audio = 0,\n",
    "                    attn_audio = 0,\n",
    "                    ltc_audio = 0,\n",
    "                    encoder_size_text = 128,        # Text\n",
    "                    num_layer_text = 1,\n",
    "                    hidden_dim_text = 200,\n",
    "                    dr_text = 1.0,\n",
    "                    dic_size = batch_gen.dic_size,\n",
    "                    use_glove = 1,\n",
    "                    bi_text = 0,\n",
    "                    attn_text = 0,\n",
    "                    ltc_text = 0,\n",
    "                    encoder_size_video = 25,    # Video\n",
    "                    num_layer_video = 1,\n",
    "                    hidden_dim_video = 128,\n",
    "                    dr_video = 1.0,\n",
    "                    bi_video = 0,\n",
    "                    attn_video = 0,\n",
    "                    ltc_video = 0\n",
    "            )\n",
    "\n",
    "model_multi.build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_encoder_inputs_audio, raw_encoder_seq_audio, raw_encoder_prosody, raw_encoder_inputs_text, raw_encoder_seq_text, raw_encoder_inputs_video, raw_encoder_seq_video, raw_label = batch_gen.get_batch(\n",
    "                            batch_size=model_multi.batch_size,\n",
    "                            data=batch_gen.train_set,\n",
    "                            encoder_size_audio=model_multi.encoder_size_audio,\n",
    "                            encoder_size_text=model_multi.encoder_size_text,\n",
    "                            encoder_size_video=model_multi.encoder_size_video,\n",
    "                            is_test=False\n",
    "                            )\n",
    "\n",
    "# prepare data which will be push from pc to placeholder\n",
    "input_feed = {}\n",
    "\n",
    "# Audio\n",
    "input_feed[model_multi.encoder_inputs_audio] = raw_encoder_inputs_audio\n",
    "input_feed[model_multi.encoder_seq_audio]    = raw_encoder_seq_audio\n",
    "input_feed[model_multi.encoder_prosody]      = raw_encoder_prosody\n",
    "input_feed[model_multi.dr_audio_in_ph]       = model_multi.dr_audio\n",
    "input_feed[model_multi.dr_audio_out_ph]      = 1.0\n",
    "\n",
    "# Text\n",
    "input_feed[model_multi.encoder_inputs_text] = raw_encoder_inputs_text\n",
    "input_feed[model_multi.encoder_seq_text]    = raw_encoder_seq_text\n",
    "input_feed[model_multi.dr_text_in_ph]       = model_multi.dr_text\n",
    "input_feed[model_multi.dr_text_out_ph]      = 1.0\n",
    "\n",
    "# Video\n",
    "input_feed[model_multi.encoder_inputs_video] = raw_encoder_inputs_video\n",
    "input_feed[model_multi.encoder_seq_video]    = raw_encoder_seq_video\n",
    "input_feed[model_multi.dr_video_in_ph]       = model_multi.dr_video\n",
    "input_feed[model_multi.dr_video_out_ph]      = 1.0\n",
    "\n",
    "# Common\n",
    "input_feed[model_multi.y_labels] = raw_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r1, r2, r3, r4 = sess.run( [model_multi.final_encoder,\n",
    "                            model_multi.final_encoder,\n",
    "                            model_multi.final_encoder,\n",
    "                            model_multi.final_encoder\n",
    "                           ],\n",
    "                          input_feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_model(model_multi, batch_gen, num_train_steps=10, valid_freq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from evaluation_AMH import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "ckpt = tf.train.get_checkpoint_state(os.path.dirname( 'save/model/' ))\n",
    "\n",
    "if ckpt and ckpt.model_checkpoint_path:\n",
    "    print ('from check point!!!')\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "else :\n",
    "    print (\"ERROR no model file in : \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ce, test_WA, test_UA, dev_summary, _ = run_test(sess=sess,\n",
    "                                                     model=model_multi, \n",
    "                                                     batch_gen=batch_gen,\n",
    "                                                     data=batch_gen.test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(test_WA, test_UA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf114_p36]",
   "language": "python",
   "name": "conda-env-tf114_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
